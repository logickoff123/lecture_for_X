
А для чего вообще нужно NLP? В чём особенность? Мы с вами уже немного обсудили классический ML и CV. В CV, например, создают и работают с изображением, которая понимает, что происходит на камере. Мой знакомый как раз работает в компании, они делают систему, которая выявляет подозрительные вещи в метро — классический ML + CV. Классический ML мы используем в банках, в торговых ботах, во всём, где нужны числа и признаки, чтобы прогнозировать и принимать решения.

А NLP работает с текстом. И текст — это, к сожалению, не пиксель и не число в начальном понимании. Зачем я вам это говорю? Когда вы обучаете модельку или уже используете готовую, она у вас как-то превращает слова в цифры. Вопрос — как?

Насколько мне известно, вас уже познакомили с такой базой как Bag of Words, он же мешок слов. Напомню, что он делает. Мы берем и выкидываем из текста всё, кроме самих слов. Мы забываем про порядок, мы забываем про пунктуацию и просто считаем.

У нас с тобой есть два отзыва:

1. «Суши вкусные и свежие, суши просто огонь.»
    
2. «Пицца холодная и невкусная.»
    

Строим словарь (все уникальные слова): `[суши, вкусные, свежие, просто, огонь, пицца, холодная, невкусная]`

Теперь превращаем каждый текст в вектор, просто считая, сколько раз каждое слово из словаря в нём встретилось:

- Отзыв 1: `[2, 1, 1, 1, 1, 0, 0, 0]` // «суши» встретилось 2 раза, остальные по разу
    
- Отзыв 2: `[0, 0, 0, 0, 0, 1, 1, 1]`
    
Мы только что превратили текст в вектор чисел. И этот вектор - уже готовые признаки (features) для нашей модели. 


Мы с вами только что сделали “мешок слов” — просто посчитали, сколько раз каждое слово встречается. Но можно задать себе вопрос:  а почему именно эти слова встретились в тексте, а не какие-то другие?

Тут и появляется идея, что **текст — это не просто набор слов, а результат случайного процесса.**  То есть, каждое слово можно рассматривать как **случайное событие**, у которого есть своя вероятность появиться.

Например:  
если мы читаем отзывы о суши, то слово “суши” будет встречаться часто — его вероятность большая.  
А слово “пицца” — реже, значит, вероятность меньше.

Можно представить, что кто-то “вытаскивает” слова из шляпы, где каждое слово лежит с определённой вероятностью:

- “суши” — 0.3
    
- “вкусные” — 0.2
    
- “свежие” — 0.15
    
- “пицца” — 0.05  
    и т.д.
    

И вот когда мы “достаём” слова одно за другим — у нас получается текст.

Мы не знаем точно, какое слово появится следующим,  но можем оценить вероятность каждого слова,  опираясь на то, что мы уже видели в текстах.

Когда мы сделали Bag of Words — мы просто посчитали, сколько раз каждое слово встретилось в тексте.  Но если подумать — это ведь и есть **приближение вероятности**.

Ведь что такое вероятность появления слова?  
Это **частота**, с которой оно встречается.  
Чем чаще слово появляется — тем выше вероятность.

Получается, когда мы строим мешок слов, мы неявно уже делаем первую простейшую вероятностную модель текста.  Просто не называем это громко “моделью”.

Например:  
в нашем наборе отзывов слово “суши” встретилось 30 раз из 100,  
а слово “пицца” — 10 раз из 100.  
Значит:

- `P(суши) = 0.3`,
    
- `P(пицца) = 0.1`.
    

Bag of Words — это просто “мешок частот”,  
Но  если поделить все частоты на общее число слов — получаем “мешок вероятностей”.
опрос зачем нам мешок веротяностей? 


Когда мы просто считаем **частоты** (Bag of Words),  мы знаем, **что** встречалось **сколько раз**.  Но эти числа _зависят от размера текста_:  в длинном отзыве слов больше, значит и частоты будут выше.

А вот **вероятности** позволяют:

1.  **Сравнивать тексты разной длины.**  
    Например, отзыв из 10 слов и отзыв из 100 слов — их нельзя сравнивать по “частоте”.  
    Но по вероятностям можно, потому что у тебя всё теперь приведено к одной шкале — от 0 до 1.

Давай на примере

- **Отзыв A (короткий)**: “Суши вкусные.”
    
- **Отзыв B (длинный)**: “Суши вкусные, свежие, недорогие, обслуживание отличное, атмосфера приятная.”

`[суши, вкусные, свежие, недорогие, обслуживание, отличное, атмосфера, приятная]`


| Слово          | Отзыв A | Отзыв B |
| -------------- | ------- | ------- |
| суши           | 1       | 1       |
| вкусные        | 1       | 1       |
| свежие         | 0       | 1       |
| недорогие      | 0       | 1       |
| обслуживание   | 0       | 1       |
| отличное       | 0       | 1       |
| атмосфера      | 0       | 1       |
| приятная       | 0       | 1       |
| **Всего слов** | **2**   | **8**   |

Если мы просто возьмём **частоты**,  то слово “суши” и там, и там встречается **один раз** —  и вроде бы одинаково важно.

Но это не так.  Потому что в коротком отзыве из двух слов “суши” = 50% текста,  
а в длинном — это всего лишь 1 из 8 слов (12.5%).

То есть **вероятности позволяют нам честно оценить важность слова относительно всего текста**,

Когда ты открываешь T9 или автодополнение в мессенджере,  он _не угадывает слово_,  он _оценивает вероятность каждого следующего слова_  на основе уже написанных.

Т.е. он думает не “угадай-ка”,  
а буквально:

 “Вероятность того, что после ‘суши’ идёт ‘вкусные’ — 0.4,   а что ‘суши’ идёт ‘холодные’ — 0.1.”


 **Мешок вероятностей** — это уже не просто подсчёт слов,  а первый шаг к тому, чтобы видеть язык как **вероятностную систему**.  Это фундамент всей дальнейшей работы — n-грамм, языковых моделей, и вообще понимания “как текст порождается”.


Мы с тобой сделали важный шаг:  теперь у нас не просто “мешок слов”, а **мешок вероятностей**.  То есть мы уже можем сказать:

“Вероятность встретить слово ‘суши’ в нашем языке — 0.08,  а слово ‘пицца’ — 0.02.”

Но есть проблема.  Если я напишу текст:

“суши вкусные пицца вкусные огонь свежие”

 Модель, построенная на “мешке слов”, не поймёт, что тут вообще происходит.  
Потому что она не знает, **в каком порядке** идут слова.А ведь порядок — это **смысл**.  
“Суши вкусные” и “вкусные суши” — то же самое,  но “суши невкусные” — уже противоположно

Чтобы не терять порядок, учёные придумали очень простую,  но гениальную идею — **n-граммы**.

- **1-грамма (unigram)** — одно слово: “суши”
    
- **2-грамма (bigram)** — пара слов: “суши вкусные”
    
- **3-грамма (trigram)** — тройка слов: “суши очень вкусны


Вероятность текста можно рассматривать  как произведение вероятностей последовательностей слов.

Например, если мы хотим оценить, насколько “естественен” текст  
“суши вкусные и свежие”,  то нам нужно узнать, насколько вероятно слово “вкусные” **после** “суши”,  и “свежие” **после** “и”.


| Последовательность | Кол-во |
| ------------------ | ------ |
| “суши вкусные”     | 20     |
| “суши невкусные”   | 3      |
| “пицца вкусная”    | 10     |
| “пицца холодная”   | 7      |


Теперь можем посчитать вероятности:

P(вкусные∣суши)=20/20+3=0.87P
P(невкусные∣суши)=3/23=0.13P

То есть — если в отзыве встретилось слово “суши”,  
то с вероятностью 87 % следующее слово будет “вкусные”.

---

Мы построили **вероятностную модель языка**,  
которая говорит: “После слова А с такой-то вероятностью идёт слово Б.”

И это уже не просто счётчик слов,  а **языковая модель**,  которая умеет предсказывать следующее слово.


- Это **основа автодополнений** (“предсказание следующего слова”).
    
- Это **основа машинного перевода** (где порядок критичен).
    
- Это **основа генерации текста** (чат-боты, нейросети и т.д.).
    


- Bag of Words — “какие слова вообще встречаются, и как часто”.
    
- N-граммы — “какие слова встречаются _вместе_, и с какой вероятностью”.
    


Векторные представление слов


Окей, у нас есть 10 000 отзывов. 

**Цель:** превратить слова в числа, чтобы с ними можно было работать математически.


1) Токенезация ( думая вам это известно просто отдлеяем слова друг от друга)

`“Суши вкусные и свежие” → [суши, вкусные, и, свежие]`



2) Удаление стоп-слов и знаков препинания (по желанию):

`[суши, вкусные, свежие]`

|Слово|Индекс|
|---|---|
|суши|0|
|вкусные|1|
|свежие|2|
|пицца|3|
|невкусные|4|
Теперь у нас есть фиксированная нумерация всех слов, с которой будем работать.

После того как мы сделали словарь:
1. Берём **каждое слово в тексте** и смотрим, какие слова рядом с ним встречаются (контекст).

2. **Считаем частоты совместного появления**:
    

Пример (контекст = слово справа):

|Слово|Сосед|Частота|
|---|---|---|
|суши|вкусные|20|
|суши|свежие|15|
|пицца|невкусные|10|



Эти частоты показывают, **как часто слово встречается рядом с другим словом**.

Чем чаще два слова появляются вместе, тем ближе они будут в векторном пространстве.


Для каждого слова делим частоты на сумму всех контекстов этого слова:

Вероятность контекста при условии слова:
$$
P(\text{контекст} | \text{слово}) = \frac{\text{частота совместного появления}}{\sum_k \text{частота всех контекстов}}
$$

**Пример для слова "суши":**

Данные:
- "суши" и "вкусные" встречаются вместе: 20 раз
- "суши" и "свежие" встречаются вместе: 15 раз

$$
\text{сумма частот} = 20 + 15 = 35
$$

$$
P(\text{вкусные}|\text{суши}) = \frac{20}{35} \approx 0.57
$$

$$
P(\text{свежие}|\text{суши}) = \frac{15}{35} \approx 0.43
$$


теперь смотри у нас есть вероятности. И как нам их предсавтить векторе. Мы ведь хотим получить вектор слов. как это сдлеать . Чтобы из вероятности перевести в вектор. Нам нужен мостик в виде логарифма. просто берем и счатем лографим по первой и второй веротяности


И тут нам на помощь приходит логарифм. Почему?

1. **Чтобы работать с суммами вместо произведений.**  
    В вероятностных моделях часто встречается умножение вероятностей. Если взять логарифм, произведение превращается в сумму — с суммами работать проще и безопаснее для компьютера.
    
2. **Чтобы маленькие вероятности не «потерялись».**  
    Если вероятность очень маленькая (например 0.001), логарифм превращает её в более удобное отрицательное число, которое всё ещё можно использовать в вычислениях.
    
3. **Чтобы получился вектор.**  
    Теперь каждое слово можно представить как **вектор логарифмов вероятностей его контекстов**:



$$
\vec{w_{суши}} = [\ln P(\text{вкусные}|\text{суши}), \ln P(\text{свежие}|\text{суши})] = [-0.56, -0.84]
$$

Этот вектор  Мы просто превратили **вероятности, с которыми слова встречаются вместе**, в числа, с которыми можно делать математику. Теперь «суши» — это точка в пространстве, где координаты отражают, какие слова обычно рядом с ним встречаются


### **Пример: полный путь от слов к сходству**

У нас есть два отзыва:

1. «Суши вкусные свежие»
    
2. «Пицца вкусная холодная»
    

**Шаг 1. Собираем словарь всех уникальных слов**

`словарь=[суши, пицца, вкусные, свежие, вкусная, холодная]`

| Слово    | Отзыв 1 | Отзыв 2 |
| -------- | ------- | ------- |
| суши     | 1       | 0       |
| пицца    | 0       | 1       |
| вкусные  | 1       | 0       |
| свежие   | 1       | 0       |
| вкусная  | 0       | 1       |
| холодная | 0       | 1       |


Превращаем частоты в вероятности

Вероятность слова = частота / сумма всех слов в отзыве

Отзыв 1: сумма = 3

𝑃(суши)=1/3≈0.33,𝑃(вкусные)=1/3≈0.33,𝑃(свежие)=1/3≈0.33

Отзыв 2: сумма = 3

𝑃(пицца)=1/3≈0.33,𝑃(вкусная)=1/3≈0.33,𝑃(холодная)=1/3≈0.33

**Берем логарифм вероятности**

ln⁡(0.33)≈−1.10

Теперь векторы слов будут такими:

- «Суши» → `[ln(0.33), ln(0.33), ln(0.33)] = [-1.10, -1.10, -1.10]`
    
- «Пицца» → `[ln(0.33), ln(0.33), ln(0.33)] = [-1.10, -1.10, -1.10]`
    

В реальности контексты для слов разные, поэтому векторы будут различаться. Мы для простоты сделали минимальный пример.


Мы хотим измерить, насколько **похожи два вектора слов** $\vec{w}_1$ и $\vec{w}_2$.

![](picture/Screenshot%20From%202025-10-15%2023-34-32.png)



Если посчитаем то получим в скаляре 3.63 в норме будет 1.91

Косинусное сходсво  0.994

Почти 1 — значит в этом упрощённом примере слова «суши» и «пицца» встречаются в похожих контекстах.

