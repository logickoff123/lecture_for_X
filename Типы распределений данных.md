
---

Дисклеймер: После того как я провела вам лекцию, я осознала что дала вам интуитивное понимание, где это используеться, но не дала вам самого важного. Вы споткнулись об слово **распредление** .  Забавно то, что я вам неформально сказала опредления данного термина. Но вы не поняли и это норм. Давайте исправим это

---

Забудем на минуту про машинное обучение и нейросети.  Представь, что тебе, как и мне в свое время, нужно выбрать тему для диплома. Есть две идеи, и обе классные. И ты не можешь решить - какую взять. Как поступает настоящий инженер?  Правильно, он дастент монетку и скажет.

 “Решка = тема №1, орёл = тема №2.”

Ты не знаешь, **что выпадет**.  Но ты точно знаешь, **что может выпасть** — либо орёл, либо решка. Вот в этот момент мы с тобой сталкиваемся с **неопределённостью**.  А чтобы описывать неопределённость, математика придумала язык — **вероятности**. 


Каждый раз когда ты подбрасываешь моенутку, результат случаен. Но если ты подбрасываешь её **много раз**, ты начинаешь видеть закономерность.

Например: за 10 бросков монетки может выпасть 4 орла. За 100 подбрасываний выпадет 50 орлов, за 1000 -  около 500.

Мы видим, что хотя каждый отдельный исход непредсказуем,  
но **частоты исходов** начинают подчиняться **определённой закономерности**.  
И вот эта закономерность - это и есть **распределение вероятностей**.


Что же вообще такое распределение? Распределение — это способ описать все возможные исходы случайного события  и присвоить каждому из них вероятность.

Для монетки у тебя будут следующие исходы 

Решка = 0.5 Орел = 0.5

Олично, теперь давай сново представим что ты работаешь в Яндекс Еде. Работаешь там разработчиком ( не курьером). И у нас есть наш любимы ресторан "Раки в Дону". У них добавилось новое меню и нам надо предсказать будет ли оно в приницпе пользоваться с просом или нет.

Так как ты не курьер, а все таки инжинер. То скажешь, что для точного понимания надо провести эксперемент

Мы возьмём 10 000 случайных пользователей в Ростове-на-Дону.

Разделим их на две группы:

**Группа A (контрольная) — видит старое приложение нового меню.

**Группа B (тестовая)** — видит новое меню “Раки в Дону”.
        

Итак, каждый пользовател  в тестовой группе может сдлеать два исхода. Он может либо купить что то из ноаого меню, либо не купить.

Задача такая же что и с монеткой, только теперь место орла и решки есть купил/не купил

Когда мы считали веротяность с монеткой, то было все просто. 1/2 = 0.5. Почему 1? потому что суммарная вероятность всех возможных исходов всегда 1. Почему два? потому что две стороны у монетки.

А как тут посчитать веротяность? Ведь заметь шанс на покупку гораздо меньше. Он не всегда будет равен половине как с монеткой. Веротяность с которой мы будем работать, число которое было взято в ходе эксперементов, мы его не придумывали а сдедали статистическое наблюдение. 

Для того что бы эт вероятность взять, нам надо посомтреть прошлую статистику за 100 человек. почему за 100? Ну потому что это это по сути и есть наша 1 в монетке. Только теперь мы будем брать это дело в процентах. Смотри, мы с тобой посмотрели прошлую статистику и увидели что похожее меню купили 5 человек из 100. Как посчитать вероятность? Верно, мы берем 5/100 = 0.05. Вот мы получили нашу вероятность которую мы будем брать в нашей задаче. если бы мы помотрели и увидели что похожее менб покупали 20 человек из 100. То мы бы 20 делили на 100. Если бы 40, то 40 делил на 100. 


Отлично мы с тобой знаем что веротяность успеха того что наше новое меню будут покупать будет 0.05. Высчитали мы это взяв прошлую статистику с походим меню

Отлично. У нас есть веротяность 0.05 у нас есть люди 10 000. ЧТо длеаем дальше ? 

А дальше мы задаемся вопросом 

А что вообще может произойти, если мы покажем это меню всем этим 10 000 людям?

- Некоторые купят, некоторые нет.  
- Кто-то посмотрит, но не нажмёт.  
- Кто-то добавит в корзину и передумает.

Но нас интересует одно:

- Сколько **в итоге** людей совершат покупку?

Если бы мы каждый раз выбирали 10 000, случайных пользоватлей то результат отличался бы 

В первом эксперементе купило бы 480 человек

Во втором 520

В третьем бы 498

И это все происходило при однйо веротяности

И вот тут нам на помощь приходим математическое ожидание. Мат ожидание говорит нам : **Сколько в среднем мы ожидаем успехов, если повторим эксперимент очень-очень много раз.**


Ситаеться это по следующей формуле 
$$
E(X) = n \cdot p = 10{,}000 \cdot 0.05 = 500
$$
Это не значит, что **всегда** купят ровно 500 человек.

Это значит, что **если бы мы проводили эксперимент тысячу раз**,  каждый раз считали, сколько человек купило,  а потом взяли среднее — то это среднее было бы примерно **500**.

Если мы подбрасываем монетку 100 раз,  
мы ожидаем примерно 50 орлов и 50 решек.  
Не ровно 50 и 50 — может быть 48 и 52, или 51 и 49,  
но **в среднем** всё равно около 50. Так вот “50” — это и есть **математическое ожидание**.



Отлично, мы знаем сколько в срднем людей может купить наше меню, знаем веротность того сколько человек покупали похожее и знаем то, что наше новое меню видят 10 ка людей.

мы с тобой нашли математическое ожидание оно же срднее. Но ведь как мы с тобой поняли что не всегда купят ровно 500. Покупок может быть 468, 390 и т.д Следоватльено мы должны с тобой найти **насколько резуьтата отклонен от срднего** И мы знаем как это длеать из прошлого урока. Мы с тобой такое проходили , что бы найти разброс от срднего нам надо использовать **дисперсию**

Формула для нашей задачи будет следующая

$$
\sigma^2 = n \cdot p \cdot (1 - p) = n \cdot p \cdot q
$$
---

p.s: так кто нибудь должен меня тэгнуть в пятницу. По идеии я закончу с формулой TF-IDF и дисперсия  биноминальная уже валяеться в айпаде. Надо будет аккуратно это все вам перенести

---

Давай подставим


$$
\sigma^2 = 10{,}000 \cdot 0.05 \cdot (1 - 0.05) = 475
$$


На прошлом уроке мы с тбой говорили про стандартное оклонения, когда ищем дисперсию в тех еденицах которые нам нужны. Для того чтобы это сдлеать надо просто дисперсию возвести под корень  

$$
sqrt(475)= 22
$$
sqrt -корень 


что в итоге 

Среднее (мат. ожидание) = 500

Разброс (стандартное отклонение) ≈ 22

Это значит, что **в большинстве экспериментов** результат будет где-то **между 478 и 522 покупками**.

Если вдруг получилось **580 покупок**,  это уже значительно выше ожидаемого - значит, **что-то реально изменилось**  (например, меню работает лучше, чем обычно, его заказло больше людей).


То что я сейчас вам показал называеться биноминальнвм распредлением. Как ее определить.

Есть **фиксированное число испытаний** n (у нас 10 000 пользователей).

Каждый результат — **независимый** от других (решение одного пользователя не влияет на другого).

У каждого испытания есть **два возможных исхода**: успех (купил) или неудача (не купил).

 Вероятность успеха p одинаковая для всех испытаний (0.05 в нашем случае).


так звучал вопрос на паре, что если вероятностей не две, а три или четыре. Вопрос хороший, и да такое бывает, для этого мы используем мультиномиальное распределение (multinomial distribution) — это обобщение биномиального на k>2k исходов.

Оно считает вероятность того, что каждый исход произойдёт определённое количество раз.

Например: из 10 000 заказов 4 000 выбрали чай, 3 000 кофе, 2 000 сок и 1 000 воду.

То есть **биномиальное = частный случай мультиномиального**, когда всего два исхода.


Подведем небольшой итог

Что нам вообще дает распрдление 

 Распрдление позволяет **предсказывать вероятности**, оценивать риски и принимать решения на данных.

Вот то что мы с тобой рассмотрели входит в калс дискретных  распредлений -- события считаются **целыми числами** (сколько человек купило, сколько орлов выпало). Каждое значение имеет определённую вероятность.

У нас есть еще непрерывное распрдление.  Что это за зверь такой

В жизни не всё так дискретно.  
Иногда нас интересует не _сколько раз_, а _насколько сильно_, _насколько быстро_, _насколько долго_.  
То есть — величины, которые могут принимать любое значение в некотором диапазоне.

##### Пример из жизни


Представь, что ты курируешь логистику в том же Яндекс Еде.  
Курьеры развозят заказы по Ростову.  
Ты хочешь узнать: сколько времени занимает доставка?

Ты начинаешь собирать данные:

- кому-то привезли за 21 минуту,
    
- кому-то за 24,
    
- кому-то за 18,
    
- кому-то за 19.5,
    
- а кто-то живёт в частном секторе и ждал 31 минуту.
    

И вот тут интересно:  время — это **не целое число**.  Оно может быть **21.1**, **21.2**, **21.23** минут и так далее.  Нельзя перечислить все возможные значения — их бесконечно много.


### Что делать, если исходов бесконечно много?

Вот тут мы и переходим от **дискретных** распределений к **непрерывным**.

В дискретном случае мы говорим:

 вероятность того, что выпадет ровно 3 орла = 0.12

А в непрерывном случае так сказать **нельзя**.  Вероятность того, что доставка заняла _ровно_ 21 минуту — практически **0**.  Зато мы можем сказать:

 вероятность того, что доставка будет от 20 до 25 минут.

Вот так и работает непрерывное распределение:  
мы не считаем отдельные значения, а смотрим на **диапазоны**.


#### График и плотность вероятности

Чтобы описывать такие распределения, математики придумали не таблицу, а **функцию** —  её называют **плотностью вероятности** (probability density function, PDF).

Эта функция показывает, где чаще встречаются значения, а где — реже.

Если ты нарисуешь график плотности времени доставки, то получишь что-то вроде **горба**:

в центре — пик (например, большинство заказов приходят за 22 минуты);

по краям — хвосты (редко, но бывает, что заказ приехал за 15 или 35 минут).




### Нормальное распределение

И вот тут на сцену выходит **нормальное распределение** 

Если ты измеришь:

- рост людей,
    
- вес котиков,
    
- время доставки,
    
- погрешности сенсоров,
    

почти всегда получится **та самая колоколообразная кривая**.

Она показывает, что:

- большинство значений сосредоточено **вокруг среднего** (мат. ожидания),
    
- чем дальше от среднего — тем реже встречается.
    

---

#### Интресный факт

Кстати, если ты вспомнишь наш пример с покупками меню “Раки в Дону”,  
там мы говорили: “в среднем 500 покупок, разброс примерно 22”.

Если количество пользователей огромное,  биномиальное распределение **становится очень похожим** на нормальное

Это даже называется **приближение нормальным распределением**.  
Именно поэтому в реальной аналитике мы часто используем нормальное распределение  — оно универсальное и работает как аппроксимация для многих дискретных случаев.

---

### Математическая форма

Формула плотности нормального распределения выглядит страшновато,  
но на деле в ней всё то же, что мы уже знаем: среднее и разброс.

$$
f(x) = \frac{1}{\sigma \sqrt{2\pi}} e^{ -\frac{(x - \mu)^2}{2\sigma^2} }
$$


Часть 1: Нормировочная константа `(1 / (σ √(2π)))`

 **Зачем она нужна?** 
 Чтобы общая площадь под всей кривой (от -∞ до +∞) была равна 1, то есть 100%. Это фундаментальное свойство распределения вероятностей.

**Что в ней важно?**
`σ` (стандартное отклонение). Чем больше `σ` (шире разброс), тем ниже и распластаннее будет кривая, чтобы площадь оставалась равной 1. Эта часть просто подстраивает высоту кривой под наш конкретный разброс.

Часть 2: Ядро экспоненты `e^( −(x − μ)² / (2σ²) )`

Это **самая главная часть формулы**, которая и задает ту самую колоколообразную форму.

`(x − μ)`: Это отклонение конкретной точки `x` от среднего значения `μ`.

 Если `x = μ` (попали ровно в среднее), то эта скобка равна 0.

`(x − μ)²`: Возводим отклонение в квадрат. Это убирает знак (нам все равно, влево или вправо отклонилось значение) и усиливает влияние больших отклонений.

`(x − μ)² / (2σ²)`:  Мы нормируем квадрат отклонения на дисперсию (`σ²`).

Минус перед дробью `- ...` : Экспонента `e^y` растет очень быстро. Чтобы она не росла, а убывала (и давала нам горб), мы берем отрицательное значение.

`e^( ... )`: Экспонента —  обеспечивает плавный, быстрый спад вероятности по мере удаления от среднего, но при этом вероятность никогда не становится равной нулю.   

####  Итог

**Вся сложная формула нужна всего лишь для того, чтобы превратить два простых параметра — `μ` (центр) и `σ` (ширину) — в красивый, плавный колокол, где:**

- Значения возле среднего — самые частые и вероятные.

- Значения дальше от среднего — всё менее вероятны.

- Скорость, с которой вероятность убывает, зависит от `σ`.

### Итого:

- **Дискретные распределения** — считаем _сколько раз что-то произошло_

- **Непрерывные распределения** — описываем _как значения распределены_ по диапазону 


И наврное самое крутое :  
Когда ты начинаешь видеть, что и то, и другое — это просто разные способы смотреть на неопределённость,  ты начинаешь думать как аналитик. 
