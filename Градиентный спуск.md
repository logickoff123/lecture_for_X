Окей, здорово! Мы с тобой поняли, что такое модель и как измерить ее ошибку. Но мы с тобой все это время подбирали те самые параметры (**`w₀` и `w₁`**) вручную. Наша цель сейчас — понять, как мы можем сделать это автоматически.

Как же найти эти самые **`w₀` и `w₁`**? Перебором это займет слишком много времени. Давай вернемся к нашему первому примеру, где мы предсказывали продажи мороженого от температуры.

**Модель:** `ŷ = w₁ * x + w₀` (прямая)

**Данные:**

|Температура (x)|Продажи (y)|
|---|---|
|20|40|
|25|70|
|30|100|
|35|140|

Наша цель, как мы выяснили, — найти такие `w₀` и `w₁`, чтобы прямая прошла ближе к точкам.

Давай вернемся и возьмем ту же подстановку, которую мы брали, только теперь взглянем на нее чуть по-другому.

**Попытка 1:** `w₁ = 1`, `w₀ = 0` (модель `y = x`)

- При x = 20: ŷ = 20 (реально 40) → **ошибка = +20**
    
- При x = 35: ŷ = 35 (реально 140) → **ошибка = +105**  
    **Вывод:** Модель сильно занижает прогнозы. Надо **увеличить** `w₁` и **увеличить** `w₀`.
    

**Попытка 2:** `w₁ = 10`, `w₀ = 0` (модель `y = 10x`)

- При x = 20: ŷ = 200 (реально 40) → **ошибка = -160**
    
- При x = 35: ŷ = 350 (реально 140) → **ошибка = -210**  
    **Вывод:** Модель сильно завышает прогнозы. Надо **уменьшить** `w₁`.
    

Сейчас мы с тобой только что вручную сделали то, что делает машина. Мы посчитали ошибку и поняли, в какую сторону надо изменить параметры.

Но машина не может "понять" или "угадать". Ей нужен четкий **алгоритм**. И такой есть — называется он **градиентный спуск**.

Давай вернемся к нашим формулам.

**Модель:** `ŷ = w₁ * x + w₀`

Функция потерь (MSE):  $$ L = \frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2 $$

И наша с тобой цель найти такое `w₀` и `w₁`, чтобы `L` → min

Теперь по сути — такой же принцип, который мы с тобой проделывали в прошлый раз. Что мы делали? Мы брали какое-то число и подставляли его в функцию потерь, брали уже предсказанное нами значение, которое есть на графике, и подставляли его туда же.

Тут все то же самое, только мы пока не берем число наугад. Но у нас точно известна наша модель, наше предсказание. Поставим эту формулу нашей модели в формулу MSE:
$$
\mathcal{L}(w_0, w_1) = \frac{1}{n} \sum_{i=1}^{n} \left[ y_i - (w_1 x_i + w_0) \right]^2
$$
Теперь наша функция потерь `L` явно **зависит от параметров**, которые мы хотим настроить.

Окей, что вот с этим нам делать дальше? Как нам в итоге найти такие значения `w₀` и `w₁`, при которых `L` минимальна?

________________________________________________________________________

**Отвлечемся от темы.** Забудем пока про градиенты, ошибки и другое. Вернемся в 11 класс, к задаче №12 на ЕГЭ по математике.

**Условие задачи:**  
Дана функция: `f(x) = x² - 4x + 3`  
Найдите её минимум.

**Как мы ее решали?**

1. Брали производную этой функции: `f'(x) = 2x - 4`.
    
2. Приравнивали ее к нулю: `2x - 4 = 0`.
    
3. Решали уравнение: `x = 2`.
    
4. Это и есть точка минимума. Подставляем `x = 2` в исходную функцию: `f(2) = 4 - 8 + 3 = -1`. Минимум равен -1.
    

**Почему это сработало?**  
Производная `f'(x)` показывает скорость изменения функции.

- Если `f'(x) > 0` — функция **растёт**.
    
- Если `f'(x) < 0` — функция **убывает**.
    
- **В точке минимума (или максимума) скорость изменения равна нулю**. Поэтому мы и приравнивали производную к нулю.
________________________________________________________________________

Окей, память освежили. В школе мы решали такие уравнения и сразу находили ответ. Это **аналитическое решение**.

Давай вернемся к нашим баранам — к функции потерь:



$$
\mathcal{L}(w_0, w_1) = \frac{1}{n} \sum_{i=1}^{n} \left[ y_i - (w_1 x_i + w_0) \right]^2
$$
Посмотри внимательно. Чем она принципиально отличается от того, что мы решали сейчас (`f(x) = x² - 4x + 3`)? Давай для удобства я заменю `x` в школьной задаче на `w₀`:

`f(w₀) = w₀² - 4w₀ +`3

А теперь посмотрим на ядро нашей функции потерь: `(y_i - (w₁x_i + w₀))²`.

Что мы видим? Правильно! В первой функции у нас **одна переменная**, а во второй — **целых две** (`w₀` и `w₁`)!

___________________________________________________________________
Я надеюсь ты понишь что $x_i$ и $y_i$ - это наши с тобой переменные которые мы с тобой знаем $y_i$ - фактическое значение, известные нам правильные ответы (продажи). (Если забыл верниьс туда где мы разбирали с тобой функцию потерь) $x_i$  - это известные нам признаки (температура) в нашем случаи .

еще раз внимание на  функции

**`f(x) = x² - 4x + 3`**

Здесь `x` — это **переменная**, которую мы меняем.
    
Наша цель — найти `x`, при котором `f(x)` минимальна.
    
Мы управляем `x`.
    
**`ℒ(w₀, w₁) = ...`**

Здесь `w₀` и `w₁` — это **параметры**, которые мы меняем.
    
`x_i` и `y_i` — это **данные!** Они (константы) для нашего датасета.
    
Наша цель — найти `w₀` и `w₁`, при которых `ℒ` минимальна.
    
Мы управляем `w₀` и `w₁`.

________________________________________________________________________
`f(w₀) = w₀² - 4w₀ + 3` — **одна переменная**.  
`ℒ(w₀, w₁) = ...` — **две переменные**!

Ситуацию это как-то меняет? Да, меняет, и очень сильно. Смотри, чтобы не уходить далеко в дебри: когда у нас **зависимость от одной переменной**, ее график — **линия на плоскости (2D)**. А когда переменных **две**, график становится **поверхностью в пространстве (3D)**.

Смотри.  
Есть функция с одной переменной — построим ее график:



![[Screenshot From 2025-09-22 22-48-25.png]]


Постороим график  нашей части ( `w₁`x₁ + `w0`)`² 

![[Screenshot From 2025-09-22 22-54-30.png]]



Вопрос закономерный: откуда у нас там взялась третья ось?

Смотри.

Когда у нас функция от **одной переменной**, то мы рисуем **график на плоскости**:

- По горизонтали откладываем `x` (вход).
    
- По вертикали откладываем `f(x)` (значение функции).  
    Получается кривая (например, парабола).
    

Когда у нас функция от **двух переменных**, то теперь «входа» у функции два — `w₀` и `w₁`. Мы уже не можем уложить всё на плоскость с осями X–Y.

Поэтому:

- По одной оси откладываем `w₀`.
    
- По второй — `w₁`.
    
- А значение функции (результат, «ошибка») откладываем «вверх» — это третья ось **Z**.
    

Таким образом, график превращается не в кривую, а в **поверхность** (чашу).

И как нам по этой чаше двигаться? Если мы будем искать минимум так, как нас учили в школе (приравнивая производную к нулю), мы его по итогу не найдем аналитически. Не забывай, я показала простую функцию (прямую), но она может быть вообще любой.

Мы подходим с тобой к тому самому алгоритму — **градиентному спуску**.

А теперь к тому, что пишут во всех учебниках и говорят на всех курсах. Данная аналогия является **столпом**, на котором стоит градиентный спуск.

**Представь, что ты стоишь в горах.** Вокруг — туман, далеко ты ничего не видишь.  
Твоя цель — добраться в самую низкую точку долины (там и будет минимум ошибки).

Что у тебя есть?

- Твои координаты (текущие значения `w₀` и `w₁`).
    
- И «инструмент» — компас, который показывает, куда склон идет **круче всего вниз**.
    

Если делать маленькие шаги в этом направлении — рано или поздно ты окажешься внизу.

Это и есть **градиентный спуск**.


- Градиент — это «стрелка компаса», только математическая.
    
- Он показывает, **в какую сторону функция растёт быстрее всего**.
    
- А нам нужно не вверх, а вниз. Поэтому берём **антиградиент** (просто меняем направление на противоположное).


Каждый шаг в горах можно записать формулой:

​$$
\mathbf{w}_{\text{new}} = \mathbf{w}_{\text{old}} - \eta \cdot \nabla \mathcal{L}(\mathbf{w}_{\text{old}})
$$


- `w_old` — где мы сейчас.
    
- `∇L` — градиент (направление самого крутого подъёма).
    
- `-` — разворот, идём вниз, а не вверх.
    
- `η` (эта греческая буква читается «эта») — длина твоего шага.
    

---

Таким образом:

- В горах — ты шагаешь по склону.
    
- В машинке — параметры `w₀` и `w₁` меняются на каждом шаге.
    
- Внизу долины — минимум функции ошибки, идеальные параметры.


Окей давай теперь поэтапно будем нашу формулу понимать 

​$$
\mathbf{w}_{\text{new}} = \mathbf{w}_{\text{old}} - \eta \cdot \nabla \mathcal{L}(\mathbf{w}_{\text{old}})
$$


Начнем  с первого. 


$$
\mathcal{L}(\mathbf{w}_{\text{old}})
$$

Это что? **Градиент**. Логично. Но что мы подразумеваем под этим символом? Давай подумаем: если градиент а) показывает направление и б) показывает, где функция растет, следовательно, градиент — это вектор производных по `w₀` и `w₁`.

Так, отлично! Но эти производные берутся от нашей функции `L`, следовательно, градиент — это `[∂L/∂w₀, ∂L/∂w₁]`. Что это значит?

- `∂w₀`, `∂w₁` указывают, по какой именно переменной мы берем производную.
    
- `∂L` указывает, какую функцию мы дифференцируем.
    

**Почему градиент — это вектор?**

**Вектор — это идеальная математическая структура, чтобы закодировать две вещи:**

1. **Направление** (куда двигаться).
    
2. **Величину** (насколько сильно двигаться в каждом направлении).
    

**Градиент `∇L = [∂L/∂w₀, ∂L/∂w₁]` — это и есть такая инструкция в виде вектора.**

**Антиградиент** — это, по сути, то же самое, только со знаком минус: `-∇L`.

Градиент указывает на подъем (нам это не нужно), а антиградиент — на спуск. С помощью знака минус мы разворачиваемся и идем к минимуму.

**Как он работает?**

1. **Встаём в случайную точку** на нашей поверхности (выбираем начальные `w₀` и `w₁` наугад).
    
2. **Оглядываемся вокруг себя** и определяем, **в какую сторону склон идёт вниз самым крутым путём**. Это направление нам и указывает **антиградиент**.
    
3. **Делаем небольшой шаг** в этом направлении.
    
4. **Повторяем** шаги 2 и 3 снова и снова, пока не окажемся в точке, из которой уже нельзя спуститься ниже.
    

---

Давай с тобой рассчитаем минимум для обычной параболы, чтобы увидеть процесс в действии.

- **Функция:** `f(x) = x² - 4x + 3`
    
- **Ее производная:** `f'(x) = 2x - 4`
    
- **Скорость обучения (шаг):** `η = 0.1`
    
- **Начальная точка:** `x_old = 5`
    

**Итерация 1:**

1. Вычисляем градиент (производную) в точке `x_old = 5`: `f'(5) = 2*5 - 4 = 10 - 4 = 6`.
    
2. Производная = 6 > 0. Чтобы функция уменьшалась, нам нужно уменьшать `x`, то есть идти влево.
    
3. Обновляем параметр, делая шаг: `x_new = x_old - η * f'(x_old) = 5 - 0.1 * 6 = 5 - 0.6 = 4.4`.
    

**Итерация 2:**

1. `x_old = 4.4`
    
2. `f'(4.4) = 2*4.4 - 4 = 8.8 - 4 = 4.8` (все еще больше нуля).
    
3. `x_new = 4.4 - 0.1 * 4.8 = 4.4 - 0.48 = 3.92`.
    

**Итерация 3:**

1. `x_old = 3.92`
    
2. `f'(3.92) = 2*3.92 - 4 = 7.84 - 4 = 3.84`
    
3. `x_new = 3.92 - 0.1 * 3.84 = 3.92 - 0.384 = 3.536`.
    

И так мы постепенно двигаемся к минимуму функции (`x = 2`)!

---

Теперь давай проделаем то же самое для нашей функции потерь `ℒ` с двумя параметрами `w₀` и `w₁`. Принцип **совершенно аналогичен**, только теперь у нас не одно, а два обновления на каждом шаге. Мы просто вычисляем частные производные по каждому параметру и обновляем их независимо, но одновременно:

$$
\begin{aligned}
w_{0_{\text{new}}} &= w_{0_{\text{old}}} - \eta \cdot \frac{\partial \mathcal{L}}{\partial w_0} \\
w_{1_{\text{new}}} &= w_{1_{\text{old}}} - \eta \cdot \frac{\partial \mathcal{L}}{\partial w_1}
\end{aligned}
$$


лгоритм градиентного спуска для линейной регрессии заключается в многократном применении этих двух формул.

1. **Инициализируем** `w₀` и `w₁` случайными значениями.
    
2. **Вычисляем** частные производные `∂ℒ/∂w₀` и `∂ℒ/∂w₁` на всех данных (или их части).
    
3. **Обновляем** параметры, делая небольшой шаг в направлении, противоположном градиенту.
    
4. **Повторяем** шаги 2 и 3 до тех пор, пока ошибка не перестанет существенно уменьшаться.
    

Таким образом, машина, не имея глаз и интуиции, а лишь слепо следуя этому математическому алгоритму, способна «нащупать» те самые наилучшие коэффициенты, которые мы вначале подбирали вручную. **Это и есть машинное обучение в действии.**


Сейчас я попытаюсь прям на пальчиках вм подробно разложить алгос и мы вместе сдлеаем первый шаг в градиентном спуске.


Но сначала, давай с тобой вообще поймем что такое chain rule. Я его на уроке пыталась вам объяснить с помощью градиента и это была моя ошибка. Учитывая что у вас матеша была на 1 курсе


Любая сложная производная, раскрываеться поочередно. Сначала внешняя, потом внутреня, потом если там есть мы раскрываем ее еще и еще. И такми образом уходит в глубины/ то есть расскрываем матрешку. Сначала большая, потом средняя потом маленькая и т.д

Теперь давай с тобой рассмотрим обычную функцию и найдем ее производную 

(3x+5)^2

- Снаружи у нас «квадрат» — это большая матрёшка.
- Внутри «3x+5» — маленькая матрёшка.

Чтобы узнать «скорость изменения всей конструкции», мы:

1. Сначала смотрим, как меняется внешняя матрёшка (квадрат).
    
2. Потом — как меняется внутренняя (3x+5).
    
3. Итоговая скорость = внешняя * внутренняя.
    

 Вот это и есть **правило цепочки**: скорость внешнего × скорость внутреннего.

Теперь смотри какой прикол ( я уже учла вопросы которые у тебя могут возникнуть, не зря же я эту лекцию два дня дописывала)))

Есть функция 
(3x+5)^2

Для удобство и для твоего понимания, заменим (3x+5) на u 

Что получим, получим что на первом шаге у нас будет 
1)y = 2u

Что будет на втором 

(3x+5) - просто 3

и в итоге у нас с тобой получилось 

y = 2*(3x+5)*3

Стоп, стоп стоп. Ну развернули производную по внешней получили двойку. Теперь по внутреней полчили 3. 2 * 3=6. Разве нет ??  
Нет))))

Почему? Потому что внешняя производная — это не просто число «2».  
Она всегда берёт ещё и саму «внутренность».  
Это как если бы ты сказала:  
«Скорость квадрата» — это не просто «удвой всё подряд»,  
а «удвой именно то, что внутри скобок».


### Представь так:

1. Внешнее (квадрат) говорит: «Возьми внутренность и умножь её на 2».  
    То есть это **2 × (3x+5)**.  
    Видишь? Тут осталась целая скобка, а не просто цифра 2.
    
2. Теперь внутренняя часть говорит: «Я сама меняюсь с коэффициентом 3».  
    То есть при каждом шаге по x, эта скобка растёт втрое быстрее.
    
3. Поэтому итог:
    


				Скорость всей функции = 2(3x+5)*3

---

### Почему скобки никуда не деваются?

Потому что функция не везде одинаково быстро растёт.  
Скорость зависит от того, какой x ты подставил:

- при x=0x: скорость = 6⋅5=30
    
- при x=10x: скорость = 6⋅35=210
    

Если бы мы тупо перемножили «2×3=6», то сказали бы: «функция всегда растёт со скоростью 6».  
Но это неправда — иногда она растёт медленно, иногда бешено быстро.  
Именно поэтому в ответе остаются скобки (3x+5).


---
### вывод маленький)

- Всегда раскрываем матрёшку снаружи внутрь.
    
- Внешняя даёт «формулу с внутренностью внутри», а не просто число.
    
- Потом умножаем на скорость внутренности.
    
- Скобки остаются, потому что именно они делают скорость «живой» и зависящей от точки.

Лови лайвхак

(x) = sin(2x)

Расскрываем матрешку с внешней функции знаем что когда расскроем внешнию внутреня к ней останеться, срауз пишем

cos(2x)

теперь раскрываем 2x. Это у нас тупо двойка которую мы перемножаем на внешнию

2 * cos(2x)
вот ты и раскрыл производную сложной функции и использовал chain rule))

Когда берешь внешнию, делаешь производную и ищешь по внешней то что идет в скобках оставляй, ведь это по сути то что тянет за собой внешняя .

Давай еще какой нибудт примерчик разберем

 f (x) = (2x^2 + 4x +8)^2

производная внешней функции

1) 2 * (2x^2 + 4x +8) - внешняя тянет за собой внутряк который идет к ней

теперь  чистая внутреняя

2) 2x^2 + 4x +8 = 4x+4

теперь по правилу цепочки перемножаем внешнию на внутрению
y = 2 * (2x^2 + 4x +8) *4x+4


Давай возьмем чето более сложное, чтобы ты понял что все не так сложно

$$
y = \ln\left( \sin\left( (x^2 + 1)^3 \right) \right)
$$



![[photo_2025-09-29_17-32-08.jpg]]



Возвращемся теперь к градиентному спуску. 

# $\mathcal{L}(w_0, w_1) = \frac{1}{n} \sum_{i=1}^{n} [ y_i - (w_1 x_i + w_0) ]^2$



Напомню что 

- xi— входные данные
    
- yi​ — реальные ответы
    
- w0,w1 — параметры модели (смещение и коэффициент)
    
- L — средняя квадратичная ошибка



Так как у нас xi и yi это данные которые нам даны уже. Давай с тобой возьмем маленький набор точек. Так тебе будет наглядно чем я буду расскладывать это по сухому без каких либо цифр 

| (xi) | (yi) |
| ---- | ---- |
| 1    | 2    |
| 2    | 3    |

Начальные парметры у наших w0 и w1 пусть будут 0 (они могут быть любыми мы их сами задаем, давай начнем с нуля, так как считаь будет гораздо легче)

Скорость обучения пусть будет n = 0.01


Окэй пока откинем сумму и другую херню работаем сейчас конкретно там где нужны парамтеры, прям как машина 


$$
[ y_i - (w_1 x_i + w_0) ]^2

$$

Производная внешней функции у нас тут будет 

$$
 2(y_i - (w_1 x_i + w_0))

$$
Давай теперь подставим наши цифирки в нашу производную по внешней функции чтобы не путаться а потом возьмем и посчитаем по внутреней уже с цифрами 


че получаем 
$$
 2(2 - (0*1  + 0)

$$

что получаем получаем 4 - это производная внешней функции и заметь внутряк мы протащили и посчитали 

Теперь давай осторожно разберемся с производной внутреней функции

$$
 (y_i - (w_1 x_i + w_0))

$$

- yᵢ = 2 → константа
    
- w₀ = 0 → константа
    
- w₁ =  это **наш параметр**, по которому берём сейчас  производную
    
- x₁ = 1 → коэффициен


Расскраываем скобки 

$$
 (2 - (w_1  1 + 0))

$$
Если сейчас расскроем скобки то что у нас останеться
$$
 (2 - w_1)

$$

Ага теперь мы берем производную по нашей скобочки которую мы расскрыли. Что осатеться  0 -1 = -1

Стоп стоп стоп, Подожди. Почему у нас w1 превратилась внезапно в 1. Мы же там задали парамтеры начальные. Откуда -1. 
Когда мы  берём производную по w₁. В этот момент **мы не подставляем w₁ = 0**, потому что мы считаем **скорость изменения функции относительно w₁**, а не значение самой функции.
    

Вот почему w₁ «исчезает» из результата: мы интересуемся **не самим w₁, а тем, как изменение w₁ влияет на функцию**.

Если подумать интуитивно: w₁ — это как **неизвестная переменная x**, по которой мы берём производную. Производная x по x = 1, а так как перед x стоит минус, получаем -1.

То же самое будет, когда мы будем брать производную по w₀: w₀ тоже «неизвестное x», и производная = 1

Именно поэтому внутренняя производная = **-1**, а x₁ остаётся как множитель, показывающий, насколько сильно изменение w₁ влияет на функцию.

То есть если место x1 = 1 у нас было 7. То у нас была бы следующая фигня

(2 - 7w1), где 2 превратилась бы в 0 так как производная константы. w1 тупо исчезло, так как мы считаем скорость изменения относительно w1, а не значение самой переменной   а 7 осталось бы . Когда мы будем брать w0 оно тоже исчезнет в производной потому что мы хотим видеть скорость изменения функции а не саму переменную.

Помнишь я на уроке проводила аналогию с рычагами. 

- **w₁** — рычаг, который отвечает за воду
    
- **w₀** — рычаг, который отвечает за свет
    
Когда мы дергаем **рычаг w₁**:

- **w₀ остаётся неизменным**, потому что мы его не трогаем, оно как константа.
    
- Мы хотим увидеть, **как изменяется струя воды**, то есть насколько изменение рычага w₁ влияет на поток.


- **Струйка воды = xᵢ** — показывает, насколько сильно движение рычага w₁ влияет на поток
    
- **w₁ — просто рычаг**, это не число и не переменная, это инструмент, чтобы измерить влияние
    
- Когда мы «повернули рычаг», мы увидели, что струя воды изменилась на -1 → это и есть **внутренняя производная**
    

#### Почему w₁ «исчезает»

- Мы использовали рычаг w₁, чтобы **измерить эффект**, а не чтобы подставлять его значение
    
- Внутренняя производная = сила и направление струи воды (-1), w₁ больше не нужен → поэтому он исчезает
    
- Теперь мы знаем, **как изменение рычага w₁ изменяет воду** (то есть скорость изменения функции по w₁)


Мы нашли внешнию производную = 4. Внутреняя производная =-1

Итого нашал w1 = -4

Давай призводную по w0 найдем 


Внешняя будет равна 2. Такой же алгоримт подсчета что и для w0

А вот теперь давай посчитаем внутрению 


$$
 (y_i - (w_1 x_i + w_0))

$$

давай сразу попробуем взять по ней производную.
yi - константа => 0
w1 - константа => 0
xi - равна => 0
w0 - то по чему мы берем производную. наш рычажок , который мы дергаем. Мы с тобой дергаем сейчас свет. И как буто бы ниче не меняться, но не забывай что перед w0 у нас стоит 1 со знаком +. 

$$
 (y_i - (w_1 x_i + 1*w_0))

$$
Ну и если мы возьмем сейчас производную все константы у нас исчезнут так как схлопнуться в 0 и останеться +1 которая стоит перед w0

По цепному правилу давай перемножим внутрению на внешнию 
внутренея у нас 1
внутреняя у нас 4
Итого : 4* 1 = 4


Вот то что мы с тобой сдлеали сейчас и есть градиент. тот самый градиент который я записывала вот так `∇L = [∂L/∂w₀, ∂L/∂w₁].  
То есть мы с тобой взяли и нашли производную w1 от L и w0 от L
∂L/∂w₀ = 4
∂L/∂w₁ = -4

А теперь давай вспомним нашу формулу 

$$
\begin{aligned}
w_{0_{\text{new}}} &= w_{0_{\text{old}}} - \eta \cdot \frac{\partial \mathcal{L}}{\partial w_0} \\
w_{1_{\text{new}}} &= w_{1_{\text{old}}} - \eta \cdot \frac{\partial \mathcal{L}}{\partial w_1}
\end{aligned}
$$

и что получим если learning rate у нас 0.01

w0_old = у нас с тобой 0. Ту точку которую мы взяли изначально

w1_old = у нас с тобой 0. Тоже с тобой взяли изначально 

теперь 
w1​:=0−0.01∗(−4) = 0+0.04=0.04 
w0:=0−0.01∗4 = 0−0.04=−0.04

И вот мы с тобой увидели как изменилась наша w1 и w0. Мы поняли что 
w1 = 0.04
w0 = - 0.04

Если мы с тобой проделаем второй шаг то наши w1_old и w0_old уже будут не 0 а 
0.04,- 0.04

По сути мы с тобой тут видим как движение каждого рычага w1 и w0 меняет результат . Мы будем повторять этот процесс шаг за шагом пока не увидим что ошибка станет минимальной

Но тут вопрос наш градиент поймет что надо осановиться 

Градиент показывает направление изменения функции потерь. Мы двигаем параметры против градиента → ошибка уменьшается
**Когда градиент ≈ 0**, значит:
    - Мы на «ровной части» поверхности функции потерь
        
    - Дальше изменение параметров почти не уменьшает ошибку
        
То есть функция потерь сама не останавливает процесс, но её форма(форма функции) и градиент говорят системе, **куда идти и когда уже почти некуда идти**


